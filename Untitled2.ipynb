{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRSoHH5Vsej5zYE4iKix2v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sohan000007/nst1/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGROIFU6yDnz"
      },
      "outputs": [],
      "source": [
        "# ─── Core Imports ─────────────────────────────────────\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "\n",
        "# ─── Custom Reflection Padding Layer ─────────────────\n",
        "class ReflectionPadding2D(layers.Layer):\n",
        "    def __init__(self, padding):\n",
        "        super().__init__()\n",
        "        if isinstance(padding, int):\n",
        "            self.padding = ((padding, padding), (padding, padding))\n",
        "        else:\n",
        "            self.padding = padding\n",
        "\n",
        "    def call(self, x):\n",
        "        return tf.pad(x, [[0, 0],\n",
        "                          [self.padding[0][0], self.padding[0][1]],\n",
        "                          [self.padding[1][0], self.padding[1][1]],\n",
        "                          [0, 0]], mode='REFLECT')\n",
        "\n",
        "# ─── Image Utilities ─────────────────────────────────\n",
        "def load_and_preprocess(path, target_size=(256, 256)):\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    img = img.resize(target_size)\n",
        "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return preprocess_input(img)\n",
        "\n",
        "def deprocess_image(x):\n",
        "    x = x.copy()\n",
        "    x[..., 0] += 103.939\n",
        "    x[..., 1] += 116.779\n",
        "    x[..., 2] += 123.68\n",
        "    x = x[..., ::-1]  # BGR to RGB\n",
        "    return np.clip(x, 0, 255).astype('uint8')\n",
        "\n",
        "# ─── Instance Normalization ──────────────────────────\n",
        "class InstanceNormalization(layers.Layer):\n",
        "    def __init__(self, epsilon=1e-5):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channels = input_shape[-1]\n",
        "        self.scale = self.add_weight(name='scale', shape=(channels,), initializer='ones')\n",
        "        self.offset = self.add_weight(name='offset', shape=(channels,), initializer='zeros')\n",
        "\n",
        "    def call(self, x):\n",
        "        mean, var = tf.nn.moments(x, [1, 2], keepdims=True)\n",
        "        x_norm = (x - mean) / tf.sqrt(var + self.epsilon)\n",
        "        return self.scale[None, None, None, :] * x_norm + self.offset[None, None, None, :]\n",
        "\n",
        "# ─── Transformer Network ─────────────────────────────\n",
        "def conv_layer(x, filters, kernel, stride):\n",
        "    pad = kernel // 2\n",
        "    x = ReflectionPadding2D(pad)(x)\n",
        "    x = layers.Conv2D(filters, kernel, strides=stride, padding='valid')(x)\n",
        "    x = InstanceNormalization()(x)\n",
        "    return layers.Activation('relu')(x)\n",
        "\n",
        "def residual_block(x, filters):\n",
        "    shortcut = x\n",
        "    y = conv_layer(x, filters, 3, 1)\n",
        "    y = ReflectionPadding2D(1)(y)\n",
        "    y = layers.Conv2D(filters, 3, strides=1, padding='valid')(y)\n",
        "    y = InstanceNormalization()(y)\n",
        "    return layers.Add()([shortcut, y])\n",
        "\n",
        "def upsample_conv(x, filters, kernel, scale):\n",
        "    x = layers.UpSampling2D(size=(scale, scale), interpolation='nearest')(x)\n",
        "    return conv_layer(x, filters, kernel, 1)\n",
        "\n",
        "def build_transformer(input_shape=(256, 256, 3)):\n",
        "    inp = layers.Input(shape=input_shape)\n",
        "    x = inp / 127.5 - 1.0\n",
        "\n",
        "    x = conv_layer(x, 32, 9, 1)\n",
        "    x = conv_layer(x, 64, 3, 2)\n",
        "    x = conv_layer(x, 128, 3, 2)\n",
        "\n",
        "    for _ in range(5):\n",
        "        x = residual_block(x, 128)\n",
        "\n",
        "    x = upsample_conv(x, 64, 3, 2)\n",
        "    x = upsample_conv(x, 32, 3, 2)\n",
        "\n",
        "    x = ReflectionPadding2D(4)(x)\n",
        "    x = layers.Conv2D(3, 9, strides=1, padding='valid', activation='tanh')(x)\n",
        "    out = (x + 1.0) * 127.5\n",
        "\n",
        "    return tf.keras.Model(inp, out, name=\"StyleTransferNet\")\n",
        "\n",
        "# ─── Build Model ─────────────────────────────────────\n",
        "model = build_transformer()\n",
        "model.summary()\n",
        "\n",
        "# ─── Load VGG for Losses ─────────────────────────────\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "STYLE_LAYERS = ['block1_conv1', 'block2_conv1', 'block3_conv1']\n",
        "CONTENT_LAYER = 'block4_conv2'\n",
        "\n",
        "vgg = VGG19(include_top=False, weights='imagenet')\n",
        "vgg.trainable = False\n",
        "outputs = [vgg.get_layer(name).output for name in STYLE_LAYERS + [CONTENT_LAYER]]\n",
        "vgg_model = Model(inputs=vgg.input, outputs=outputs)\n",
        "\n",
        "# ─── Loss Functions ──────────────────────────────────\n",
        "def content_loss(generated, target):\n",
        "    return tf.reduce_mean(tf.square(generated - target))\n",
        "\n",
        "def style_loss(gen_feats, style_feats):\n",
        "    loss = 0.0\n",
        "    for gf, sf in zip(gen_feats, style_feats):\n",
        "        loss += tf.reduce_mean(tf.square(gf - sf))\n",
        "    return loss\n",
        "\n",
        "def total_variation_loss(x):\n",
        "    return tf.image.total_variation(x)\n",
        "\n",
        "# ─── Optimizer & Weights ─────────────────────────────\n",
        "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
        "α, β, γ = 0.001, 0.001, 1e-6\n",
        "\n",
        "# ─── Load Style Image ────────────────────────────────\n",
        "style_path = '/content/drive/MyDrive/Sohan/images.jpg'  # Replace with your style image path\n",
        "style_tensor = load_and_preprocess(style_path)\n",
        "style_outputs = vgg_model(style_tensor)\n",
        "style_targets = style_outputs[:len(STYLE_LAYERS)]\n",
        "\n",
        "# ─── Training Step ───────────────────────────────────\n",
        "def train_step(content_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        gen_imgs = model(content_batch)\n",
        "        gen_feats = vgg_model(gen_imgs)\n",
        "        style_feats = gen_feats[:len(STYLE_LAYERS)]\n",
        "        content_feat = gen_feats[-1]\n",
        "\n",
        "        vgg_content = vgg_model(content_batch)[-1]\n",
        "\n",
        "        c_loss = content_loss(content_feat, vgg_content)\n",
        "        s_loss = style_loss(style_feats, style_targets)\n",
        "        tv_loss = total_variation_loss(gen_imgs)\n",
        "        total = α * c_loss + β * s_loss + γ * tv_loss\n",
        "\n",
        "    grads = tape.gradient(total, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return total\n",
        "\n",
        "# ─── Stylisation Utility ─────────────────────────────\n",
        "def stylise_and_show(img_path):\n",
        "    img = load_and_preprocess(img_path)\n",
        "    output = model(img, training=False)\n",
        "    output = output.numpy()[0]\n",
        "    out_img = deprocess_image(output)\n",
        "    plt.imshow(out_img)\n",
        "    plt.axis('off')\n",
        "    plt.title('Stylised Output')\n",
        "    plt.show()\n",
        "\n",
        "# ─── Example Usage ───────────────────────────────────\n",
        "content_path = '/content/drive/MyDrive/Sohan/download.jpg'  # Replace with your content image path\n",
        "content_tensor = load_and_preprocess(content_path)\n",
        "\n",
        "# Train for a few steps\n",
        "for i in range(100):  # Increase this number for better quality\n",
        "    loss = train_step(content_tensor)\n",
        "    print(f\"Step {i + 1}: Loss = {loss.numpy().item():.4f}\")\n",
        "\n",
        "# Show stylised result\n",
        "stylise_and_show(content_path)\n"
      ]
    }
  ]
}